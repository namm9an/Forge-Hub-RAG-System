import { createSupabaseClient } from '../supabase/client';
import { geminiEmbeddingService } from './geminiEmbeddings';
import { vectorStorageService } from '../vectors/vectorStorage';
import { 
  EmbeddingJob, 
  EmbeddingJobProgress, 
  DocumentChunk, 
  Embedding,
  EmbeddingBatchRequest,
  EmbeddingBatchResponse
} from '../types';

export class EmbeddingProcessor {
  private supabase: ReturnType<typeof createSupabaseClient>;
  private processing: Set<string> = new Set();

  constructor() {
    this.supabase = createSupabaseClient();
  }

  /**
   * Process document chunks to generate embeddings
   */
  async processDocumentChunks(documentId: string): Promise<string> {
    if (this.processing.has(documentId)) {
      throw new Error(`Document ${documentId} is already being processed`);
    }

    try {
      this.processing.add(documentId);

      // Get document chunks
      const { data: chunks, error: chunksError } = await this.supabase
        .from('document_chunks')
        .select('id, content, chunk_index, metadata')
        .eq('document_id', documentId)
        .order('chunk_index', { ascending: true });

      if (chunksError) throw chunksError;
      if (!chunks || chunks.length === 0) {
        throw new Error('No chunks found for document');
      }

      // Create embedding job
      const jobId = await this.createEmbeddingJob(documentId, chunks.map(c => c.id));

      // Process in background
      this.processBatchEmbeddings(chunks, jobId).catch(error => {
        console.error(`Background processing failed for job ${jobId}:`, error);
        this.updateJobStatus(jobId, 'failed', error.message);
      });

      return jobId;

    } finally {
      this.processing.delete(documentId);
    }
  }

  /**
   * Process batch of embeddings
   */
  async processBatchEmbeddings(chunks: DocumentChunk[], jobId: string): Promise<void> {
    const batchSize = 10;
    const batches = this.createBatches(chunks, batchSize);
    
    await this.updateJobStatus(jobId, 'processing');

    let processedCount = 0;
    let failedCount = 0;

    for (let i = 0; i < batches.length; i++) {
      const batch = batches[i];
      console.log(`Processing batch ${i + 1}/${batches.length} for job ${jobId}`);

      try {
        const embeddings: Embedding[] = [];
        
        for (const chunk of batch) {
          try {
            // Check if embedding already exists
            const { data: existingEmbedding } = await this.supabase
              .from('embeddings')
              .select('id')
              .eq('chunk_id', chunk.id)
              .single();

            if (existingEmbedding) {
              console.log(`Embedding already exists for chunk ${chunk.id}`);
              processedCount++;
              continue;
            }

            // Generate embedding
            const response = await geminiEmbeddingService.generateEmbedding({
              text: chunk.content,
              cache_result: true
            });

            const embedding: Embedding = {
              id: '', // Will be generated by database
              chunk_id: chunk.id,
              embedding: response.embedding,
              model_version: response.model_version,
              embedding_metadata: response.metadata,
              processing_time_ms: response.processing_time_ms,
              similarity_cache: {},
              status: 'completed',
              retry_count: 0,
              created_at: new Date().toISOString(),
              updated_at: new Date().toISOString()
            };

            embeddings.push(embedding);
            processedCount++;

          } catch (error) {
            console.error(`Failed to process chunk ${chunk.id}:`, error);
            failedCount++;
            
            // Store failed embedding record
            await this.storeFailedEmbedding(chunk.id, error);
          }
        }

        // Batch store successful embeddings
        if (embeddings.length > 0) {
          await vectorStorageService.batchStoreEmbeddings(embeddings);
        }

        // Update job progress
        await this.updateJobProgress(jobId, processedCount, failedCount);

        // Rate limiting delay
        if (i < batches.length - 1) {
          await this.delay(4000); // 4 second delay between batches
        }

      } catch (error) {
        console.error(`Batch ${i + 1} failed:`, error);
        failedCount += batch.length;
        await this.updateJobProgress(jobId, processedCount, failedCount);
      }
    }

    // Complete job
    const finalStatus = failedCount === 0 ? 'completed' : 
                       processedCount === 0 ? 'failed' : 'completed';
    
    await this.updateJobStatus(jobId, finalStatus);
  }

  /**
   * Create embedding job record
   */
  private async createEmbeddingJob(documentId: string, chunkIds: string[]): Promise<string> {
    const estimatedCompletion = new Date();
    estimatedCompletion.setMinutes(estimatedCompletion.getMinutes() + Math.ceil(chunkIds.length / 10) * 5);

    const { data, error } = await this.supabase
      .from('embedding_jobs')
      .insert({
        document_id: documentId,
        chunk_ids: chunkIds,
        status: 'pending',
        total_chunks: chunkIds.length,
        estimated_completion: estimatedCompletion.toISOString(),
        processing_metadata: {
          model_version: 'text-embedding-004',
          batch_size: 10
        }
      })
      .select('id')
      .single();

    if (error) throw error;
    return data.id;
  }

  /**
   * Update job status
   */
  private async updateJobStatus(jobId: string, status: string, errorMessage?: string): Promise<void> {
    const updateData: any = { status };
    
    if (status === 'processing') {
      updateData.started_at = new Date().toISOString();
    } else if (status === 'completed' || status === 'failed') {
      updateData.completed_at = new Date().toISOString();
    }
    
    if (errorMessage) {
      updateData.error_message = errorMessage;
    }

    await this.supabase
      .from('embedding_jobs')
      .update(updateData)
      .eq('id', jobId);
  }

  /**
   * Update job progress
   */
  private async updateJobProgress(jobId: string, processedCount: number, failedCount: number): Promise<void> {
    await this.supabase
      .from('embedding_jobs')
      .update({
        processed_chunks: processedCount,
        failed_chunks: failedCount
      })
      .eq('id', jobId);
  }

  /**
   * Store failed embedding record
   */
  private async storeFailedEmbedding(chunkId: string, error: any): Promise<void> {
    const failedEmbedding: Embedding = {
      id: '', // Will be generated by database
      chunk_id: chunkId,
      embedding: [], // Empty embedding for failed cases
      model_version: 'text-embedding-004',
      embedding_metadata: {},
      similarity_cache: {},
      status: 'failed',
      error_message: error instanceof Error ? error.message : String(error),
      retry_count: 0,
      created_at: new Date().toISOString(),
      updated_at: new Date().toISOString()
    };

    try {
      await this.supabase
        .from('embeddings')
        .insert(failedEmbedding);
    } catch (insertError) {
      console.error('Failed to store failed embedding record:', insertError);
    }
  }

  /**
   * Get embedding job status
   */
  async getEmbeddingJobStatus(jobId: string): Promise<EmbeddingJobProgress | null> {
    const { data, error } = await this.supabase
      .from('embedding_job_progress')
      .select('*')
      .eq('id', jobId)
      .single();

    if (error) return null;
    return data as EmbeddingJobProgress;
  }

  /**
   * Get all embedding jobs for a document
   */
  async getDocumentEmbeddingJobs(documentId: string): Promise<EmbeddingJobProgress[]> {
    const { data, error } = await this.supabase
      .from('embedding_job_progress')
      .select('*')
      .eq('document_id', documentId)
      .order('created_at', { ascending: false });

    if (error) return [];
    return data as EmbeddingJobProgress[];
  }

  /**
   * Retry failed embeddings
   */
  async retryFailedEmbeddings(jobId: string): Promise<void> {
    // Get failed chunks from the job
    const { data: job, error: jobError } = await this.supabase
      .from('embedding_jobs')
      .select('document_id, chunk_ids')
      .eq('id', jobId)
      .single();

    if (jobError) throw jobError;

    // Get failed embeddings
    const { data: failedEmbeddings, error: failedError } = await this.supabase
      .from('embeddings')
      .select('chunk_id')
      .in('chunk_id', job.chunk_ids)
      .eq('status', 'failed');

    if (failedError) throw failedError;

    if (failedEmbeddings && failedEmbeddings.length > 0) {
      const failedChunkIds = failedEmbeddings.map(e => e.chunk_id);
      
      // Get chunks for retry
      const { data: chunks, error: chunksError } = await this.supabase
        .from('document_chunks')
        .select('id, content, chunk_index, metadata')
        .in('id', failedChunkIds);

      if (chunksError) throw chunksError;
      if (chunks && chunks.length > 0) {
        await this.processBatchEmbeddings(chunks, jobId);
      }
    }
  }

  /**
   * Cancel embedding job
   */
  async cancelEmbeddingJob(jobId: string): Promise<void> {
    await this.supabase
      .from('embedding_jobs')
      .update({
        status: 'cancelled',
        completed_at: new Date().toISOString()
      })
      .eq('id', jobId);
  }

  /**
   * Process batch embedding request
   */
  async processBatchRequest(request: EmbeddingBatchRequest): Promise<EmbeddingBatchResponse> {
    const jobs: EmbeddingJob[] = [];
    let totalChunks = 0;

    for (const documentId of request.document_ids) {
      try {
        const jobId = await this.processDocumentChunks(documentId);
        
        // Get job details
        const jobStatus = await this.getEmbeddingJobStatus(jobId);
        if (jobStatus) {
          jobs.push({
            id: jobId,
            document_id: documentId,
            chunk_ids: [], // Will be populated by actual processing
            status: jobStatus.status,
            batch_size: request.batch_size || 10,
            total_chunks: jobStatus.total_chunks,
            processed_chunks: jobStatus.processed_chunks,
            failed_chunks: jobStatus.failed_chunks,
            estimated_completion: jobStatus.estimated_completion || '',
            processing_metadata: {},
            retry_count: jobStatus.retry_count,
            created_at: new Date().toISOString(),
            updated_at: new Date().toISOString()
          });
          
          totalChunks += jobStatus.total_chunks;
        }
      } catch (error) {
        console.error(`Failed to process document ${documentId}:`, error);
      }
    }

    const estimatedCompletion = new Date();
    estimatedCompletion.setMinutes(estimatedCompletion.getMinutes() + Math.ceil(totalChunks / 10) * 5);

    return {
      job_id: `batch_${Date.now()}`,
      total_documents: request.document_ids.length,
      total_chunks: totalChunks,
      estimated_completion: estimatedCompletion.toISOString(),
      batch_jobs: jobs
    };
  }

  /**
   * Utility functions
   */
  private createBatches<T>(array: T[], batchSize: number): T[][] {
    const batches: T[][] = [];
    for (let i = 0; i < array.length; i += batchSize) {
      batches.push(array.slice(i, i + batchSize));
    }
    return batches;
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// Export singleton instance
export const embeddingProcessor = new EmbeddingProcessor();
export default embeddingProcessor;
